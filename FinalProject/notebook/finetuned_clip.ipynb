{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor, AdamW, get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "data_dir = \"/root/20242R0136COSE47402/FinalProject/data/train\"\n",
    "class_candidate = [folder for folder in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder))]\n",
    "text_inputs = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if os.path.isdir(os.path.join(data_dir, folder)):\n",
    "        folder = folder.replace('_', ' ')\n",
    "        text_inputs.append(f\"a photo of {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size : 75750\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "image_labels = []\n",
    "\n",
    "for class_name in class_candidate:\n",
    "    class_folder = os.path.join(data_dir, class_name)\n",
    "    for img_name in os.listdir(class_folder):\n",
    "        if img_name[0] == '.':\n",
    "            continue\n",
    "        img_path = os.path.join(class_folder, img_name)\n",
    "        image_paths.append(img_path)\n",
    "        class_name = class_name.replace('_', ' ')\n",
    "        image_labels.append(f\"a photo of {class_name}\")\n",
    "\n",
    "print(f\"train dataset size : {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "\n",
    "    input_ids = [inp['input_ids'] for inp in inputs]\n",
    "    attention_masks = [inp['attention_mask'] for inp in inputs]\n",
    "    pixel_values = [inp['pixel_values'] for inp in inputs]\n",
    "\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "    batch_inputs = {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_masks_padded,\n",
    "        'pixel_values': pixel_values\n",
    "    }\n",
    "\n",
    "    return batch_inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_loss(logits_per_image):\n",
    "    targets = torch.arange(len(logits_per_image), device=logits_per_image.device)\n",
    "\n",
    "    return (F.cross_entropy(logits_per_image, targets) + F.cross_entropy(logits_per_image.T, targets)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Food101DataSet(Dataset):\n",
    "    def __init__(self, image_paths, image_labels, processor):\n",
    "        self.image_paths = image_paths\n",
    "        self.image_labels = image_labels\n",
    "        self.processor = processor\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.image_labels[idx]\n",
    "        inputs = self.processor(text=label, images=image, return_tensors='pt')\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs, label\n",
    "\n",
    "train_dataset = Food101DataSet(image_paths, image_labels, processor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.67 GiB of which 17.56 MiB is free. Process 1011094 has 5.89 GiB memory in use. Process 1011730 has 5.73 GiB memory in use. Of the allocated memory 5.39 GiB is allocated by PyTorch, and 374.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3163\u001b[0m         )\n\u001b[0;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1327\u001b[0m         device,\n\u001b[1;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1329\u001b[0m         non_blocking,\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.67 GiB of which 17.56 MiB is free. Process 1011094 has 5.89 GiB memory in use. Process 1011730 has 5.73 GiB memory in use. Of the allocated memory 5.39 GiB is allocated by PyTorch, and 374.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "epochs = 12\n",
    "num_warmup_steps = int(0.1 * len(train_dataloader)) * epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=len(train_dataloader) * epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 12:   0%|          | 1/4735 [00:00<40:49,  1.93it/s, loss=0.641]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.67 GiB of which 19.56 MiB is free. Process 1011094 has 5.89 GiB memory in use. Process 1011730 has 5.73 GiB memory in use. Of the allocated memory 5.39 GiB is allocated by PyTorch, and 374.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m batch_inputs, labels \u001b[38;5;241m=\u001b[39m batch \n\u001b[1;32m      8\u001b[0m batch_inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch_inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch_inputs)\n\u001b[1;32m     12\u001b[0m logits_per_image \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits_per_image\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m clip_loss(logits_per_image)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:1363\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m   1358\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1359\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1360\u001b[0m )\n\u001b[1;32m   1361\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1363\u001b[0m vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model(\n\u001b[1;32m   1364\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[1;32m   1365\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1366\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1367\u001b[0m     interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding,\n\u001b[1;32m   1368\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1369\u001b[0m )\n\u001b[1;32m   1371\u001b[0m text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_model(\n\u001b[1;32m   1372\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1373\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1378\u001b[0m )\n\u001b[1;32m   1380\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:1097\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m   1094\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[1;32m   1095\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m-> 1097\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1098\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m   1099\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1100\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1101\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1102\u001b[0m )\n\u001b[1;32m   1104\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1105\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:877\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    869\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    870\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    871\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    874\u001b[0m         output_attentions,\n\u001b[1;32m    875\u001b[0m     )\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m encoder_layer(\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[1;32m    879\u001b[0m         attention_mask,\n\u001b[1;32m    880\u001b[0m         causal_attention_mask,\n\u001b[1;32m    881\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    882\u001b[0m     )\n\u001b[1;32m    884\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:608\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    605\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    607\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 608\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    609\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    610\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    611\u001b[0m     causal_attention_mask\u001b[38;5;241m=\u001b[39mcausal_attention_mask,\n\u001b[1;32m    612\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    613\u001b[0m )\n\u001b[1;32m    614\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    616\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:540\u001b[0m, in \u001b[0;36mCLIPSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    537\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# CLIP text model uses both `causal_attention_mask` and `attention_mask` sequentially.\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m    541\u001b[0m     query_states,\n\u001b[1;32m    542\u001b[0m     key_states,\n\u001b[1;32m    543\u001b[0m     value_states,\n\u001b[1;32m    544\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m    545\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    546\u001b[0m     scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale,\n\u001b[1;32m    547\u001b[0m )\n\u001b[1;32m    549\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    550\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, embed_dim)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.67 GiB of which 19.56 MiB is free. Process 1011094 has 5.89 GiB memory in use. Process 1011730 has 5.73 GiB memory in use. Of the allocated memory 5.39 GiB is allocated by PyTorch, and 374.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1} / {epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        batch_inputs, labels = batch \n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "\n",
    "        outputs = model(**batch_inputs)\n",
    "\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        loss = clip_loss(logits_per_image)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    print(f\"epoch {epoch + 1} finished.\")\n",
    "\n",
    "print(\"Train completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_store_path = \"/root/20242R0136COSE47402/FinalProject/src/origin_clip_loss.txt\"\n",
    "with open (loss_store_path, 'w') as file:\n",
    "    for loss in losses:\n",
    "        file.write(f\"{loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "data_dir = \"/root/20242R0136COSE47402/FinalProject/data/test\"\n",
    "class_candidate = [folder for folder in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder))]\n",
    "text_inputs = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if os.path.isdir(os.path.join(data_dir, folder)):\n",
    "        folder = folder.replace('_', ' ')\n",
    "        text_inputs.append(f\"a photo of {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset size : 25250\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "image_labels = []\n",
    "\n",
    "for class_name in class_candidate:\n",
    "    class_folder = os.path.join(data_dir, class_name)\n",
    "    for img_name in os.listdir(class_folder):\n",
    "        if img_name[0] == '.':\n",
    "            continue\n",
    "        img_path = os.path.join(class_folder, img_name)\n",
    "        image_paths.append(img_path)\n",
    "        class_name = class_name.replace('_', ' ')\n",
    "        image_labels.append(f\"a photo of {class_name}\")\n",
    "\n",
    "print(f\"test dataset size : {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0 finished.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_labels[idx]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m ans \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(image_paths) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     18\u001b[0m accuracy\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "ans = 0\n",
    "model.eval()\n",
    "\n",
    "for idx, img_path in enumerate(image_paths):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    inputs = processor(text=text_inputs, images=image, return_tensors='pt', padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()} \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    if text_inputs[torch.argmax(probs).item()] == image_labels[idx]:\n",
    "        ans += 1\n",
    "    if idx % 2000 == 0:\n",
    "        print(f\"# {idx} finished.\")\n",
    "        \n",
    "accuracy = ans / len(image_paths) * 100\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl3_2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
