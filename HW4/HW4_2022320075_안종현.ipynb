{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5Jm3UN_Hfsu"
   },
   "source": [
    "## **Homework 4**\n",
    "**Instructions**\n",
    "* This homework focuses on understanding and applying CoCoOp for CLIP prompt tuning. It consists of **four questions** designed to assess both theoretical understanding and practical application.\n",
    "\n",
    "* Please organize your answers and results for the questions below and submit this jupyter notebook as **a .pdf file**.\n",
    "\n",
    "* **Deadline: 11/26 (Sat) 23:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeRABv42Ku4E"
   },
   "source": [
    "### **Preparation**\n",
    "\n",
    "* Run the code below before proceeding with the homework.\n",
    "* If an error occurs, click ‘Run Session Again’ and then restart the runtime from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNOsgBEzKucv",
    "outputId": "9f61082d-c209-45be-c75a-59ff0173fab4"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/mlvlab/ProMetaR.git\n",
    "%cd ProMetaR/\n",
    "\n",
    "!git clone https://github.com/KaiyangZhou/Dassl.pytorch.git\n",
    "%cd Dassl.pytorch/\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "!cp -r dassl ../\n",
    "# Install this library (no need to re-build if the source code is modified)\n",
    "# !python setup.py develop\n",
    "%cd ..\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "%mkdir outputs\n",
    "%mkdir data\n",
    "\n",
    "%cd data\n",
    "%mkdir eurosat\n",
    "!wget http://madm.dfki.de/files/sentinel/EuroSAT.zip -O EuroSAT.zip\n",
    "\n",
    "!unzip -o EuroSAT.zip -d eurosat/\n",
    "%cd eurosat\n",
    "!gdown 1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o\n",
    "\n",
    "%cd ../../\n",
    "\n",
    "import os.path as osp\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import argparse\n",
    "from dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
    "from dassl.config import get_cfg_default\n",
    "from dassl.engine import build_trainer\n",
    "from dassl.engine import TRAINER_REGISTRY, TrainerX\n",
    "from dassl.metrics import compute_accuracy\n",
    "from dassl.utils import load_pretrained_weights, load_checkpoint\n",
    "from dassl.optim import build_optimizer, build_lr_scheduler\n",
    "\n",
    "# custom\n",
    "import datasets.oxford_pets\n",
    "import datasets.oxford_flowers\n",
    "import datasets.fgvc_aircraft\n",
    "import datasets.dtd\n",
    "import datasets.eurosat\n",
    "import datasets.stanford_cars\n",
    "import datasets.food101\n",
    "import datasets.sun397\n",
    "import datasets.caltech101\n",
    "import datasets.ucf101\n",
    "import datasets.imagenet\n",
    "import datasets.imagenet_sketch\n",
    "import datasets.imagenetv2\n",
    "import datasets.imagenet_a\n",
    "import datasets.imagenet_r\n",
    "\n",
    "def print_args(args, cfg):\n",
    "    print(\"***************\")\n",
    "    print(\"** Arguments **\")\n",
    "    print(\"***************\")\n",
    "    optkeys = list(args.__dict__.keys())\n",
    "    optkeys.sort()\n",
    "    for key in optkeys:\n",
    "        print(\"{}: {}\".format(key, args.__dict__[key]))\n",
    "    print(\"************\")\n",
    "    print(\"** Config **\")\n",
    "    print(\"************\")\n",
    "    print(cfg)\n",
    "\n",
    "def reset_cfg(cfg, args):\n",
    "    if args.root:\n",
    "        cfg.DATASET.ROOT = args.root\n",
    "    if args.output_dir:\n",
    "        cfg.OUTPUT_DIR = args.output_dir\n",
    "    if args.seed:\n",
    "        cfg.SEED = args.seed\n",
    "    if args.trainer:\n",
    "        cfg.TRAINER.NAME = args.trainer\n",
    "    cfg.DATASET.NUM_SHOTS = 16\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = args.subsample_classes\n",
    "    cfg.DATALOADER.TRAIN_X.BATCH_SIZE = args.train_batch_size\n",
    "    cfg.OPTIM.MAX_EPOCH = args.epoch\n",
    "\n",
    "def extend_cfg(cfg):\n",
    "    \"\"\"\n",
    "    Add new config variables.\n",
    "    \"\"\"\n",
    "    from yacs.config import CfgNode as CN\n",
    "    cfg.TRAINER.COOP = CN()\n",
    "    cfg.TRAINER.COOP.N_CTX = 16  # number of context vectors\n",
    "    cfg.TRAINER.COOP.CSC = False  # class-specific context\n",
    "    cfg.TRAINER.COOP.CTX_INIT = \"\"  # initialization words\n",
    "    cfg.TRAINER.COOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.COOP.CLASS_TOKEN_POSITION = \"end\"  # 'middle' or 'end' or 'front'\n",
    "    cfg.TRAINER.COCOOP = CN()\n",
    "    cfg.TRAINER.COCOOP.N_CTX = 4  # number of context vectors\n",
    "    cfg.TRAINER.COCOOP.CTX_INIT = \"a photo of a\"  # initialization words\n",
    "    cfg.TRAINER.COCOOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.PROMETAR = CN()\n",
    "    cfg.TRAINER.PROMETAR.N_CTX_VISION = 4  # number of context vectors at the vision branch\n",
    "    cfg.TRAINER.PROMETAR.N_CTX_TEXT = 4  # number of context vectors at the language branch\n",
    "    cfg.TRAINER.PROMETAR.CTX_INIT = \"a photo of a\"  # initialization words\n",
    "    cfg.TRAINER.PROMETAR.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_VISION = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
    "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_TEXT = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = \"all\"  # all, base or new\n",
    "    cfg.TRAINER.PROMETAR.ADAPT_LR = 0.0005\n",
    "    cfg.TRAINER.PROMETAR.LR_RATIO = 0.0005\n",
    "    cfg.TRAINER.PROMETAR.FAST_ADAPTATION = False\n",
    "    cfg.TRAINER.PROMETAR.MIXUP_ALPHA = 0.5\n",
    "    cfg.TRAINER.PROMETAR.MIXUP_BETA = 0.5\n",
    "    cfg.TRAINER.PROMETAR.DIM_RATE=8\n",
    "    cfg.OPTIM_VNET = CN()\n",
    "    cfg.OPTIM_VNET.NAME = \"adam\"\n",
    "    cfg.OPTIM_VNET.LR = 0.0003\n",
    "    cfg.OPTIM_VNET.WEIGHT_DECAY = 5e-4\n",
    "    cfg.OPTIM_VNET.MOMENTUM = 0.9\n",
    "    cfg.OPTIM_VNET.SGD_DAMPNING = 0\n",
    "    cfg.OPTIM_VNET.SGD_NESTEROV = False\n",
    "    cfg.OPTIM_VNET.RMSPROP_ALPHA = 0.99\n",
    "    cfg.OPTIM_VNET.ADAM_BETA1 = 0.9\n",
    "    cfg.OPTIM_VNET.ADAM_BETA2 = 0.999\n",
    "    cfg.OPTIM_VNET.STAGED_LR = False\n",
    "    cfg.OPTIM_VNET.NEW_LAYERS = ()\n",
    "    cfg.OPTIM_VNET.BASE_LR_MULT = 0.1\n",
    "    # Learning rate scheduler\n",
    "    cfg.OPTIM_VNET.LR_SCHEDULER = \"single_step\"\n",
    "    # -1 or 0 means the stepsize is equal to max_epoch\n",
    "    cfg.OPTIM_VNET.STEPSIZE = (-1, )\n",
    "    cfg.OPTIM_VNET.GAMMA = 0.1\n",
    "    cfg.OPTIM_VNET.MAX_EPOCH = 10\n",
    "    # Set WARMUP_EPOCH larger than 0 to activate warmup training\n",
    "    cfg.OPTIM_VNET.WARMUP_EPOCH = -1\n",
    "    # Either linear or constant\n",
    "    cfg.OPTIM_VNET.WARMUP_TYPE = \"linear\"\n",
    "    # Constant learning rate when type=constant\n",
    "    cfg.OPTIM_VNET.WARMUP_CONS_LR = 1e-5\n",
    "    # Minimum learning rate when type=linear\n",
    "    cfg.OPTIM_VNET.WARMUP_MIN_LR = 1e-5\n",
    "    # Recount epoch for the next scheduler (last_epoch=-1)\n",
    "    # Otherwise last_epoch=warmup_epoch\n",
    "    cfg.OPTIM_VNET.WARMUP_RECOUNT = True\n",
    "\n",
    "def setup_cfg(args):\n",
    "    cfg = get_cfg_default()\n",
    "    extend_cfg(cfg)\n",
    "    # 1. From the dataset config file\n",
    "    if args.dataset_config_file:\n",
    "        cfg.merge_from_file(args.dataset_config_file)\n",
    "    # 2. From the method config file\n",
    "    if args.config_file:\n",
    "        cfg.merge_from_file(args.config_file)\n",
    "    # 3. From input arguments\n",
    "    reset_cfg(cfg, args)\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def load_clip_to_cpu(cfg): # Load CLIP\n",
    "    backbone_name = cfg.MODEL.BACKBONE.NAME\n",
    "    url = clip._MODELS[backbone_name]\n",
    "    model_path = clip._download(url)\n",
    "\n",
    "    try:\n",
    "        # loading JIT archive\n",
    "        model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
    "        state_dict = None\n",
    "\n",
    "    except RuntimeError:\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "    if cfg.TRAINER.NAME == \"\":\n",
    "      design_trainer = \"CoOp\"\n",
    "    else:\n",
    "      design_trainer = cfg.TRAINER.NAME\n",
    "    design_details = {\"trainer\": design_trainer,\n",
    "                      \"vision_depth\": 0,\n",
    "                      \"language_depth\": 0, \"vision_ctx\": 0,\n",
    "                      \"language_ctx\": 0}\n",
    "    model = clip.build_model(state_dict or model.state_dict(), design_details)\n",
    "\n",
    "    return model\n",
    "\n",
    "from dassl.config import get_cfg_default\n",
    "cfg = get_cfg_default()\n",
    "cfg.MODEL.BACKBONE.NAME = \"ViT-B/16\" # Set the vision encoder backbone of CLIP to ViT.\n",
    "clip_model = load_clip_to_cpu(cfg)\n",
    "\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model): # 초기화 하는 함수\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts): # 모델 호출\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "@TRAINER_REGISTRY.register(force=True)\n",
    "class CoCoOp(TrainerX):\n",
    "    def check_cfg(self, cfg):\n",
    "        assert cfg.TRAINER.COCOOP.PREC in [\"fp16\", \"fp32\", \"amp\"]\n",
    "\n",
    "    def build_model(self):\n",
    "        cfg = self.cfg\n",
    "        classnames = self.dm.dataset.classnames\n",
    "        print(f\"Loading CLIP (backbone: {cfg.MODEL.BACKBONE.NAME})\")\n",
    "        clip_model = load_clip_to_cpu(cfg)\n",
    "\n",
    "        if cfg.TRAINER.COCOOP.PREC == \"fp32\" or cfg.TRAINER.COCOOP.PREC == \"amp\":\n",
    "            # CLIP's default precision is fp16\n",
    "            clip_model.float()\n",
    "\n",
    "        print(\"Building custom CLIP\")\n",
    "        self.model = CoCoOpCustomCLIP(cfg, classnames, clip_model)\n",
    "\n",
    "        print(\"Turning off gradients in both the image and the text encoder\")\n",
    "        name_to_update = \"prompt_learner\"\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name_to_update not in name:\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        # Double check\n",
    "        enabled = set()\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                enabled.add(name)\n",
    "        print(f\"Parameters to be updated: {enabled}\")\n",
    "\n",
    "        if cfg.MODEL.INIT_WEIGHTS:\n",
    "            load_pretrained_weights(self.model.prompt_learner, cfg.MODEL.INIT_WEIGHTS)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        # NOTE: only give prompt_learner to the optimizer\n",
    "        self.optim = build_optimizer(self.model.prompt_learner, cfg.OPTIM)\n",
    "        self.sched = build_lr_scheduler(self.optim, cfg.OPTIM)\n",
    "        self.register_model(\"prompt_learner\", self.model.prompt_learner, self.optim, self.sched)\n",
    "\n",
    "        self.scaler = GradScaler() if cfg.TRAINER.COCOOP.PREC == \"amp\" else None\n",
    "\n",
    "        # Note that multi-gpu training could be slow because CLIP's size is\n",
    "        # big, which slows down the copy operation in DataParallel\n",
    "        device_count = torch.cuda.device_count()\n",
    "        if device_count > 1:\n",
    "            print(f\"Multiple GPUs detected (n_gpus={device_count}), use all of them!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "    def before_train(self):\n",
    "        directory = self.cfg.OUTPUT_DIR\n",
    "        if self.cfg.RESUME:\n",
    "            directory = self.cfg.RESUME\n",
    "        self.start_epoch = self.resume_model_if_exist(directory)\n",
    "\n",
    "        # Remember the starting time (for computing the elapsed time)\n",
    "        self.time_start = time.time()\n",
    "\n",
    "\n",
    "    def forward_backward(self, batch):\n",
    "        image, label = self.parse_batch_train(batch)\n",
    "\n",
    "        model = self.model\n",
    "        optim = self.optim\n",
    "        scaler = self.scaler\n",
    "\n",
    "        prec = self.cfg.TRAINER.COCOOP.PREC\n",
    "        loss = model(image, label) # Input image 모델 통과\n",
    "        optim.zero_grad()\n",
    "        loss.backward() # Backward (역전파)\n",
    "        optim.step() # 모델 parameter update\n",
    "\n",
    "        loss_summary = {\"loss\": loss.item()}\n",
    "\n",
    "        if (self.batch_idx + 1) == self.num_batches:\n",
    "            self.update_lr()\n",
    "\n",
    "        return loss_summary\n",
    "\n",
    "    def parse_batch_train(self, batch):\n",
    "        input = batch[\"img\"]\n",
    "        label = batch[\"label\"]\n",
    "        input = input.to(self.device)\n",
    "        label = label.to(self.device)\n",
    "        return input, label\n",
    "\n",
    "    def load_model(self, directory, epoch=None):\n",
    "        if not directory:\n",
    "            print(\"Note that load_model() is skipped as no pretrained model is given\")\n",
    "            return\n",
    "\n",
    "        names = self.get_model_names()\n",
    "\n",
    "        # By default, the best model is loaded\n",
    "        model_file = \"model-best.pth.tar\"\n",
    "\n",
    "        if epoch is not None:\n",
    "            model_file = \"model.pth.tar-\" + str(epoch)\n",
    "\n",
    "        for name in names:\n",
    "            model_path = osp.join(directory, name, model_file)\n",
    "\n",
    "            if not osp.exists(model_path):\n",
    "                raise FileNotFoundError('Model not found at \"{}\"'.format(model_path))\n",
    "\n",
    "            checkpoint = load_checkpoint(model_path)\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "            epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "            # Ignore fixed token vectors\n",
    "            if \"token_prefix\" in state_dict:\n",
    "                del state_dict[\"token_prefix\"]\n",
    "\n",
    "            if \"token_suffix\" in state_dict:\n",
    "                del state_dict[\"token_suffix\"]\n",
    "\n",
    "            print(\"Loading weights to {} \" 'from \"{}\" (epoch = {})'.format(name, model_path, epoch))\n",
    "            # set strict=False\n",
    "            self._models[name].load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    def after_train(self):\n",
    "      print(\"Finish training\")\n",
    "\n",
    "      do_test = not self.cfg.TEST.NO_TEST\n",
    "      if do_test:\n",
    "          if self.cfg.TEST.FINAL_MODEL == \"best_val\":\n",
    "              print(\"Deploy the model with the best val performance\")\n",
    "              self.load_model(self.output_dir)\n",
    "          else:\n",
    "              print(\"Deploy the last-epoch model\")\n",
    "          acc = self.test()\n",
    "\n",
    "      # Show elapsed time\n",
    "      elapsed = round(time.time() - self.time_start)\n",
    "      elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "      print(f\"Elapsed: {elapsed}\")\n",
    "\n",
    "      # Close writer\n",
    "      self.close_writer()\n",
    "      return acc\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Generic training loops.\"\"\"\n",
    "        self.before_train()\n",
    "        for self.epoch in range(self.start_epoch, self.max_epoch):\n",
    "            self.before_epoch()\n",
    "            self.run_epoch()\n",
    "            self.after_epoch()\n",
    "        acc = self.after_train()\n",
    "        return acc\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--root\", type=str, default=\"data/\", help=\"path to dataset\")\n",
    "parser.add_argument(\"--output-dir\", type=str, default=\"outputs/cocoop3\", help=\"output directory\")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=1, help=\"only positive value enables a fixed seed\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config-file\", type=str, default=\"configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\", help=\"path to config file\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset-config-file\",\n",
    "    type=str,\n",
    "    default=\"configs/datasets/eurosat.yaml\",\n",
    "    help=\"path to config file for dataset setup\",\n",
    ")\n",
    "parser.add_argument(\"--trainer\", type=str, default=\"CoOp\", help=\"name of trainer\")\n",
    "parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"evaluation only\")\n",
    "parser.add_argument(\n",
    "    \"--model-dir\",\n",
    "    type=str,\n",
    "    default=\"\",\n",
    "    help=\"load model from this directory for eval-only mode\",\n",
    ")\n",
    "parser.add_argument(\"--train-batch-size\", type=int, default=4)\n",
    "parser.add_argument(\"--epoch\", type=int, default=10)\n",
    "parser.add_argument(\"--subsample-classes\", type=str, default=\"base\")\n",
    "parser.add_argument(\n",
    "    \"--load-epoch\", type=int, default=0, help=\"load model weights at this epoch for evaluation\"\n",
    ")\n",
    "args = parser.parse_args([])\n",
    "\n",
    "def main(args):\n",
    "    cfg = setup_cfg(args)\n",
    "    if cfg.SEED >= 0:\n",
    "        set_random_seed(cfg.SEED)\n",
    "\n",
    "    if torch.cuda.is_available() and cfg.USE_CUDA:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    trainer = build_trainer(cfg)\n",
    "    if args.eval_only:\n",
    "        trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
    "        acc = trainer.test()\n",
    "        return acc\n",
    "\n",
    "    acc = trainer.train()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3n9blo4JO7m"
   },
   "source": [
    "### **Q1.  Understanding and implementing CoCoOp**\n",
    "* We have learned how to define CoOp in Lab Session 4.\n",
    "\n",
    "* The main difference between CoOp and CoCoOp is **meta network** to extract image tokens that is added to the text prompt.\n",
    "\n",
    "* Based on the CoOp code given in Lab Session 4, fill-in-the-blank exercise (4 blanks!!) to test your understanding of critical parts of the CoCoOp.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SONlVIhPH_qF"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CoCoOpPromptLearner(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        n_ctx = cfg.TRAINER.COCOOP.N_CTX\n",
    "        ctx_init = cfg.TRAINER.COCOOP.CTX_INIT\n",
    "        dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "        cfg_imsize = cfg.INPUT.SIZE[0]\n",
    "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})\"\n",
    "\n",
    "        if ctx_init:\n",
    "            # use given words to initialize context vectors\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1: 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # random initialization\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors)  # Wrap the initialized prompts above as parameters to make them trainable.\n",
    "\n",
    "        ### Tokenize ###\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]  # 예) \"Forest\"\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames] # 예) \"A photo of Forest.\"\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]) # 예) [49406, 320, 1125, 539...]\n",
    "\n",
    "\n",
    "\n",
    "        #####################################\n",
    "        ####### Q1. Fill in the blank #######\n",
    "        ########## Define Meta Net ##########\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
    "        ]))\n",
    "        #####################################\n",
    "        ## Hint: meta network is composed to linear layer, relu activation, and linear layer.\n",
    "\n",
    "\n",
    "\n",
    "        if cfg.TRAINER.COCOOP.PREC == \"fp16\":\n",
    "            self.meta_net.half()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx:, :])  # CLS, EOS\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
    "        self.name_lens = name_lens\n",
    "\n",
    "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
    "        # dim0 is either batch_size (during training) or n_cls (during testing)\n",
    "        # ctx: context tokens, with shape of (dim0, n_ctx, ctx_dim)\n",
    "        # prefix: the sos token, with shape of (n_cls, 1, ctx_dim)\n",
    "        # suffix: remaining tokens, with shape of (n_cls, *, ctx_dim)\n",
    "\n",
    "        if label is not None:\n",
    "            prefix = prefix[label]\n",
    "            suffix = suffix[label]\n",
    "\n",
    "        prompts = torch.cat(\n",
    "            [\n",
    "                prefix,  # (dim0, 1, dim)\n",
    "                ctx,  # (dim0, n_ctx, dim)\n",
    "                suffix,  # (dim0, *, dim)\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx  # (n_ctx, ctx_dim)\n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ########## Q2,3. Fill in the blank #########\n",
    "        bias = self.meta_net(im_features)  # (batch, ctx_dim)\n",
    "        bias = bias.unsqueeze(1)  # (batch, 1, ctx_dim)\n",
    "        ctx = ctx.unsqueeze(0)  # (1, n_ctx, ctx_dim)\n",
    "        ctx_shifted = ctx + bias  # (batch, n_ctx, ctx_dim)\n",
    "        ############################################\n",
    "        ############################################\n",
    "\n",
    "\n",
    "\n",
    "        # Use instance-conditioned context tokens for all classes\n",
    "        prompts = []\n",
    "        for ctx_shifted_i in ctx_shifted:\n",
    "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tkn, ctx_dim)\n",
    "            prompts.append(pts_i)\n",
    "        prompts = torch.stack(prompts)\n",
    "\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L_BluKEdKA94"
   },
   "outputs": [],
   "source": [
    "class CoCoOpCustomCLIP(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = CoCoOpPromptLearner(cfg, classnames, clip_model)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, image, label=None):\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ########## Q4. Fill in the blank #########\n",
    "        prompts = self.prompt_learner(image_features)\n",
    "        ############################################\n",
    "        ############################################\n",
    "\n",
    "\n",
    "        logits = []\n",
    "        for pts_i, imf_i in zip(prompts, image_features):\n",
    "            text_features = self.text_encoder(pts_i, tokenized_prompts)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            l_i = logit_scale * imf_i @ text_features.t()\n",
    "            logits.append(l_i)\n",
    "        logits = torch.stack(logits)\n",
    "\n",
    "        if self.prompt_learner.training:\n",
    "            return F.cross_entropy(logits, label)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CGZlqo-HtRN"
   },
   "source": [
    "### **Q2. Trainining CoCoOp**\n",
    "\n",
    "In this task, you will train CoCoOp on the EuroSAT dataset. If your implementation of CoCoOp in Question 1 is correct, the following code should execute without errors. Please submit the execution file so we can evaluate whether your code runs without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zy3bAMnBMrXP",
    "outputId": "da60b888-0337-45d2-b1a0-339168c158a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer: CoCoOp\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/ProMetaR/data/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/ProMetaR/data/eurosat/split_fewshot/shot_16-seed_1.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  5\n",
      "# train_x  80\n",
      "# val      20\n",
      "# test     4,200\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building custom CLIP\n",
      "Initial context: \"a photo of a\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.meta_net.linear1.bias', 'prompt_learner.meta_net.linear2.weight', 'prompt_learner.ctx', 'prompt_learner.meta_net.linear1.weight', 'prompt_learner.meta_net.linear2.bias'}\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100] batch [20/20] time 0.146 (0.300) data 0.000 (0.031) loss 0.2744 (1.1881) lr 2.5000e-03 eta 0:09:53\n",
      "epoch [2/100] batch [20/20] time 0.100 (0.139) data 0.000 (0.026) loss 0.8384 (0.8970) lr 2.4994e-03 eta 0:04:31\n",
      "epoch [3/100] batch [20/20] time 0.098 (0.133) data 0.000 (0.023) loss 0.6382 (0.7859) lr 2.4975e-03 eta 0:04:17\n",
      "epoch [4/100] batch [20/20] time 0.106 (0.128) data 0.000 (0.017) loss 0.5044 (0.7151) lr 2.4945e-03 eta 0:04:04\n",
      "epoch [5/100] batch [20/20] time 0.155 (0.158) data 0.000 (0.019) loss 0.5703 (0.6317) lr 2.4901e-03 eta 0:05:00\n",
      "epoch [6/100] batch [20/20] time 0.099 (0.154) data 0.000 (0.037) loss 0.6060 (0.6009) lr 2.4846e-03 eta 0:04:48\n",
      "epoch [7/100] batch [20/20] time 0.099 (0.140) data 0.000 (0.029) loss 0.3853 (0.6638) lr 2.4779e-03 eta 0:04:20\n",
      "epoch [8/100] batch [20/20] time 0.099 (0.130) data 0.000 (0.023) loss 1.4082 (0.6633) lr 2.4699e-03 eta 0:03:59\n",
      "epoch [9/100] batch [20/20] time 0.168 (0.152) data 0.000 (0.017) loss 0.1780 (0.4582) lr 2.4607e-03 eta 0:04:36\n",
      "epoch [10/100] batch [20/20] time 0.143 (0.206) data 0.000 (0.041) loss 1.2285 (0.5051) lr 2.4504e-03 eta 0:06:10\n",
      "epoch [11/100] batch [20/20] time 0.097 (0.129) data 0.000 (0.020) loss 0.2539 (0.5013) lr 2.4388e-03 eta 0:03:50\n",
      "epoch [12/100] batch [20/20] time 0.094 (0.144) data 0.000 (0.034) loss 1.1484 (0.4657) lr 2.4261e-03 eta 0:04:13\n",
      "epoch [13/100] batch [20/20] time 0.097 (0.133) data 0.000 (0.025) loss 0.8467 (0.5009) lr 2.4122e-03 eta 0:03:52\n",
      "epoch [14/100] batch [20/20] time 0.139 (0.205) data 0.000 (0.019) loss 0.5547 (0.4495) lr 2.3972e-03 eta 0:05:53\n",
      "epoch [15/100] batch [20/20] time 0.095 (0.132) data 0.000 (0.021) loss 1.0430 (0.5549) lr 2.3810e-03 eta 0:03:44\n",
      "epoch [16/100] batch [20/20] time 0.098 (0.132) data 0.000 (0.020) loss 1.3906 (0.4799) lr 2.3638e-03 eta 0:03:41\n",
      "epoch [17/100] batch [20/20] time 0.100 (0.129) data 0.000 (0.017) loss 0.0238 (0.3497) lr 2.3454e-03 eta 0:03:33\n",
      "epoch [18/100] batch [20/20] time 0.147 (0.154) data 0.000 (0.017) loss 0.1337 (0.2804) lr 2.3259e-03 eta 0:04:11\n",
      "epoch [19/100] batch [20/20] time 0.168 (0.208) data 0.000 (0.045) loss 1.0420 (0.3864) lr 2.3054e-03 eta 0:05:37\n",
      "epoch [20/100] batch [20/20] time 0.103 (0.133) data 0.000 (0.022) loss 0.3484 (0.4984) lr 2.2839e-03 eta 0:03:32\n",
      "epoch [21/100] batch [20/20] time 0.099 (0.139) data 0.000 (0.022) loss 0.8184 (0.3434) lr 2.2613e-03 eta 0:03:39\n",
      "epoch [22/100] batch [20/20] time 0.110 (0.130) data 0.000 (0.020) loss 0.2090 (0.4361) lr 2.2377e-03 eta 0:03:23\n",
      "epoch [23/100] batch [20/20] time 0.140 (0.159) data 0.000 (0.017) loss 0.0860 (0.3027) lr 2.2131e-03 eta 0:04:04\n",
      "epoch [24/100] batch [20/20] time 0.095 (0.137) data 0.000 (0.034) loss 0.1953 (0.5398) lr 2.1876e-03 eta 0:03:28\n",
      "epoch [25/100] batch [20/20] time 0.100 (0.132) data 0.000 (0.022) loss 0.3467 (0.3832) lr 2.1612e-03 eta 0:03:18\n",
      "epoch [26/100] batch [20/20] time 0.099 (0.131) data 0.000 (0.018) loss 0.2426 (0.3475) lr 2.1339e-03 eta 0:03:13\n",
      "epoch [27/100] batch [20/20] time 0.126 (0.146) data 0.000 (0.017) loss 0.2076 (0.3300) lr 2.1057e-03 eta 0:03:33\n",
      "epoch [28/100] batch [20/20] time 0.138 (0.197) data 0.000 (0.028) loss 0.3286 (0.3118) lr 2.0766e-03 eta 0:04:43\n",
      "epoch [29/100] batch [20/20] time 0.098 (0.131) data 0.000 (0.022) loss 0.5962 (0.3705) lr 2.0468e-03 eta 0:03:06\n",
      "epoch [30/100] batch [20/20] time 0.100 (0.130) data 0.000 (0.021) loss 0.0064 (0.3970) lr 2.0161e-03 eta 0:03:02\n",
      "epoch [31/100] batch [20/20] time 0.098 (0.133) data 0.000 (0.022) loss 0.7725 (0.3600) lr 1.9847e-03 eta 0:03:03\n",
      "epoch [32/100] batch [20/20] time 0.147 (0.158) data 0.000 (0.020) loss 0.1853 (0.2766) lr 1.9526e-03 eta 0:03:34\n",
      "epoch [33/100] batch [20/20] time 0.100 (0.142) data 0.000 (0.027) loss 0.1498 (0.2754) lr 1.9198e-03 eta 0:03:09\n",
      "epoch [34/100] batch [20/20] time 0.101 (0.131) data 0.000 (0.019) loss 0.0809 (0.3418) lr 1.8863e-03 eta 0:02:53\n",
      "epoch [35/100] batch [20/20] time 0.097 (0.131) data 0.000 (0.019) loss 0.0195 (0.2815) lr 1.8522e-03 eta 0:02:50\n",
      "epoch [36/100] batch [20/20] time 0.121 (0.144) data 0.000 (0.023) loss 0.1168 (0.2929) lr 1.8175e-03 eta 0:03:04\n",
      "epoch [37/100] batch [20/20] time 0.141 (0.204) data 0.000 (0.029) loss 0.2375 (0.3413) lr 1.7822e-03 eta 0:04:16\n",
      "epoch [38/100] batch [20/20] time 0.096 (0.134) data 0.000 (0.027) loss 0.3904 (0.2160) lr 1.7464e-03 eta 0:02:45\n",
      "epoch [39/100] batch [20/20] time 0.097 (0.132) data 0.000 (0.018) loss 0.0148 (0.2756) lr 1.7102e-03 eta 0:02:40\n",
      "epoch [40/100] batch [20/20] time 0.104 (0.137) data 0.000 (0.025) loss 0.0934 (0.3496) lr 1.6734e-03 eta 0:02:44\n",
      "epoch [41/100] batch [20/20] time 0.149 (0.161) data 0.000 (0.018) loss 0.2169 (0.2247) lr 1.6363e-03 eta 0:03:09\n",
      "epoch [42/100] batch [20/20] time 0.096 (0.138) data 0.000 (0.032) loss 0.2345 (0.3324) lr 1.5987e-03 eta 0:02:39\n",
      "epoch [43/100] batch [20/20] time 0.097 (0.131) data 0.000 (0.027) loss 1.5879 (0.2942) lr 1.5609e-03 eta 0:02:29\n",
      "epoch [44/100] batch [20/20] time 0.096 (0.129) data 0.000 (0.020) loss 0.0948 (0.2610) lr 1.5227e-03 eta 0:02:24\n",
      "epoch [45/100] batch [20/20] time 0.121 (0.145) data 0.000 (0.020) loss 0.0528 (0.2330) lr 1.4842e-03 eta 0:02:39\n",
      "epoch [46/100] batch [20/20] time 0.163 (0.207) data 0.000 (0.035) loss 0.0851 (0.3257) lr 1.4455e-03 eta 0:03:43\n",
      "epoch [47/100] batch [20/20] time 0.105 (0.132) data 0.000 (0.030) loss 0.8765 (0.2289) lr 1.4067e-03 eta 0:02:20\n",
      "epoch [48/100] batch [20/20] time 0.097 (0.131) data 0.000 (0.018) loss 0.1407 (0.2187) lr 1.3676e-03 eta 0:02:16\n",
      "epoch [49/100] batch [20/20] time 0.103 (0.130) data 0.000 (0.024) loss 0.2174 (0.2130) lr 1.3285e-03 eta 0:02:12\n",
      "epoch [50/100] batch [20/20] time 0.153 (0.166) data 0.000 (0.019) loss 0.5039 (0.2274) lr 1.2893e-03 eta 0:02:46\n",
      "epoch [51/100] batch [20/20] time 0.108 (0.142) data 0.000 (0.036) loss 0.1648 (0.2948) lr 1.2500e-03 eta 0:02:18\n",
      "epoch [52/100] batch [20/20] time 0.100 (0.129) data 0.000 (0.021) loss 0.1514 (0.2735) lr 1.2107e-03 eta 0:02:04\n",
      "epoch [53/100] batch [20/20] time 0.095 (0.129) data 0.000 (0.026) loss 0.3259 (0.2046) lr 1.1715e-03 eta 0:02:01\n",
      "epoch [54/100] batch [20/20] time 0.128 (0.147) data 0.000 (0.021) loss 0.1115 (0.2189) lr 1.1324e-03 eta 0:02:15\n",
      "epoch [55/100] batch [20/20] time 0.140 (0.202) data 0.000 (0.032) loss 0.2917 (0.1799) lr 1.0933e-03 eta 0:03:02\n",
      "epoch [56/100] batch [20/20] time 0.099 (0.133) data 0.000 (0.018) loss 0.2384 (0.2613) lr 1.0545e-03 eta 0:01:56\n",
      "epoch [57/100] batch [20/20] time 0.095 (0.134) data 0.000 (0.025) loss 0.3364 (0.3352) lr 1.0158e-03 eta 0:01:55\n",
      "epoch [58/100] batch [20/20] time 0.098 (0.132) data 0.000 (0.018) loss 0.3237 (0.2660) lr 9.7732e-04 eta 0:01:50\n",
      "epoch [59/100] batch [20/20] time 0.145 (0.162) data 0.000 (0.022) loss 0.0295 (0.2851) lr 9.3914e-04 eta 0:02:12\n",
      "epoch [60/100] batch [20/20] time 0.140 (0.196) data 0.000 (0.038) loss 0.0961 (0.1896) lr 9.0126e-04 eta 0:02:37\n",
      "epoch [61/100] batch [20/20] time 0.097 (0.132) data 0.000 (0.020) loss 0.3149 (0.2265) lr 8.6373e-04 eta 0:01:42\n",
      "epoch [62/100] batch [20/20] time 0.095 (0.131) data 0.000 (0.020) loss 0.0041 (0.2124) lr 8.2658e-04 eta 0:01:39\n",
      "epoch [63/100] batch [20/20] time 0.096 (0.132) data 0.000 (0.022) loss 0.1748 (0.2624) lr 7.8984e-04 eta 0:01:37\n",
      "epoch [64/100] batch [20/20] time 0.144 (0.158) data 0.000 (0.023) loss 0.2600 (0.1714) lr 7.5357e-04 eta 0:01:53\n",
      "epoch [65/100] batch [20/20] time 0.108 (0.145) data 0.000 (0.037) loss 0.5747 (0.2100) lr 7.1778e-04 eta 0:01:41\n",
      "epoch [66/100] batch [20/20] time 0.096 (0.132) data 0.000 (0.020) loss 0.1279 (0.1686) lr 6.8251e-04 eta 0:01:29\n",
      "epoch [67/100] batch [20/20] time 0.100 (0.131) data 0.000 (0.019) loss 0.0054 (0.2219) lr 6.4781e-04 eta 0:01:26\n",
      "epoch [68/100] batch [20/20] time 0.155 (0.151) data 0.000 (0.018) loss 0.2773 (0.2684) lr 6.1370e-04 eta 0:01:36\n",
      "epoch [69/100] batch [20/20] time 0.142 (0.208) data 0.000 (0.033) loss 0.0228 (0.2471) lr 5.8022e-04 eta 0:02:08\n",
      "epoch [70/100] batch [20/20] time 0.096 (0.131) data 0.000 (0.022) loss 0.2318 (0.1503) lr 5.4740e-04 eta 0:01:18\n",
      "epoch [71/100] batch [20/20] time 0.105 (0.134) data 0.000 (0.022) loss 0.0285 (0.1188) lr 5.1527e-04 eta 0:01:17\n",
      "epoch [72/100] batch [20/20] time 0.096 (0.131) data 0.000 (0.023) loss 0.1163 (0.2144) lr 4.8387e-04 eta 0:01:13\n",
      "epoch [73/100] batch [20/20] time 0.171 (0.159) data 0.000 (0.025) loss 0.0424 (0.1745) lr 4.5322e-04 eta 0:01:25\n",
      "epoch [74/100] batch [20/20] time 0.098 (0.142) data 0.000 (0.033) loss 0.1774 (0.1305) lr 4.2336e-04 eta 0:01:14\n",
      "epoch [75/100] batch [20/20] time 0.115 (0.135) data 0.000 (0.025) loss 0.0523 (0.1880) lr 3.9432e-04 eta 0:01:07\n",
      "epoch [76/100] batch [20/20] time 0.102 (0.133) data 0.000 (0.019) loss 0.0109 (0.1781) lr 3.6612e-04 eta 0:01:03\n",
      "epoch [77/100] batch [20/20] time 0.150 (0.156) data 0.000 (0.024) loss 0.0092 (0.1832) lr 3.3879e-04 eta 0:01:11\n",
      "epoch [78/100] batch [20/20] time 0.099 (0.207) data 0.000 (0.037) loss 0.1420 (0.2149) lr 3.1236e-04 eta 0:01:31\n",
      "epoch [79/100] batch [20/20] time 0.099 (0.132) data 0.000 (0.019) loss 0.6455 (0.2502) lr 2.8686e-04 eta 0:00:55\n",
      "epoch [80/100] batch [20/20] time 0.108 (0.132) data 0.000 (0.027) loss 0.1262 (0.1671) lr 2.6231e-04 eta 0:00:52\n",
      "epoch [81/100] batch [20/20] time 0.098 (0.133) data 0.000 (0.018) loss 0.1049 (0.1736) lr 2.3873e-04 eta 0:00:50\n",
      "epoch [82/100] batch [20/20] time 0.174 (0.170) data 0.000 (0.021) loss 0.5278 (0.1947) lr 2.1615e-04 eta 0:01:01\n",
      "epoch [83/100] batch [20/20] time 0.098 (0.133) data 0.000 (0.028) loss 0.1053 (0.1895) lr 1.9459e-04 eta 0:00:45\n",
      "epoch [84/100] batch [20/20] time 0.098 (0.134) data 0.000 (0.020) loss 0.1261 (0.1526) lr 1.7407e-04 eta 0:00:42\n",
      "epoch [85/100] batch [20/20] time 0.108 (0.137) data 0.000 (0.019) loss 0.0314 (0.1640) lr 1.5462e-04 eta 0:00:40\n",
      "epoch [86/100] batch [20/20] time 0.139 (0.160) data 0.000 (0.024) loss 0.0459 (0.1491) lr 1.3624e-04 eta 0:00:44\n",
      "epoch [87/100] batch [20/20] time 0.110 (0.143) data 0.000 (0.036) loss 0.2108 (0.1862) lr 1.1897e-04 eta 0:00:37\n",
      "epoch [88/100] batch [20/20] time 0.099 (0.139) data 0.000 (0.025) loss 0.1178 (0.2581) lr 1.0281e-04 eta 0:00:33\n",
      "epoch [89/100] batch [20/20] time 0.108 (0.133) data 0.000 (0.026) loss 0.0460 (0.2158) lr 8.7779e-05 eta 0:00:29\n",
      "epoch [90/100] batch [20/20] time 0.152 (0.167) data 0.000 (0.019) loss 0.0492 (0.1039) lr 7.3899e-05 eta 0:00:33\n",
      "epoch [91/100] batch [20/20] time 0.101 (0.203) data 0.000 (0.032) loss 0.2791 (0.1459) lr 6.1179e-05 eta 0:00:36\n",
      "epoch [92/100] batch [20/20] time 0.098 (0.138) data 0.000 (0.028) loss 0.0514 (0.1019) lr 4.9633e-05 eta 0:00:22\n",
      "epoch [93/100] batch [20/20] time 0.120 (0.149) data 0.000 (0.023) loss 0.1763 (0.2449) lr 3.9271e-05 eta 0:00:20\n",
      "epoch [94/100] batch [20/20] time 0.113 (0.152) data 0.000 (0.034) loss 0.2859 (0.2261) lr 3.0104e-05 eta 0:00:18\n",
      "epoch [95/100] batch [20/20] time 0.160 (0.218) data 0.000 (0.037) loss 0.1564 (0.1853) lr 2.2141e-05 eta 0:00:21\n",
      "epoch [96/100] batch [20/20] time 0.111 (0.148) data 0.000 (0.027) loss 0.4089 (0.1330) lr 1.5390e-05 eta 0:00:11\n",
      "epoch [97/100] batch [20/20] time 0.116 (0.161) data 0.000 (0.041) loss 0.0698 (0.1542) lr 9.8566e-06 eta 0:00:09\n",
      "epoch [98/100] batch [20/20] time 0.109 (0.149) data 0.000 (0.029) loss 0.2188 (0.2041) lr 5.5475e-06 eta 0:00:05\n",
      "epoch [99/100] batch [20/20] time 0.144 (0.220) data 0.000 (0.041) loss 0.0691 (0.1264) lr 2.4666e-06 eta 0:00:04\n",
      "epoch [100/100] batch [20/20] time 0.107 (0.141) data 0.000 (0.025) loss 0.0025 (0.1101) lr 6.1680e-07 eta 0:00:00\n",
      "Checkpoint saved to outputs/cocoop/prompt_learner/model.pth.tar-100\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [01:06<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 4,200\n",
      "* correct: 3,813\n",
      "* accuracy: 90.8%\n",
      "* error: 9.2%\n",
      "* macro_f1: 90.9%\n",
      "Elapsed: 0:06:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train on the Base Classes Train split and evaluate accuracy on the Base Classes Test split.\n",
    "args.trainer = \"CoCoOp\"\n",
    "args.train_batch_size = 4\n",
    "args.epoch = 100\n",
    "args.output_dir = \"outputs/cocoop\"\n",
    "\n",
    "args.subsample_classes = \"base\"\n",
    "args.eval_only = False\n",
    "cocoop_base_acc = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xql7WpJ5vPII",
    "outputId": "2a6bc13d-0012-419e-fe58-c239f716629b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer: CoCoOp\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /content/ProMetaR/data/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /content/ProMetaR/data/eurosat/split_fewshot/shot_16-seed_1.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  5\n",
      "# train_x  80\n",
      "# val      20\n",
      "# test     3,900\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/content/ProMetaR/dassl/utils/torchtools.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fpath, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building custom CLIP\n",
      "Initial context: \"a photo of a\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.meta_net.linear1.bias', 'prompt_learner.meta_net.linear2.weight', 'prompt_learner.ctx', 'prompt_learner.meta_net.linear1.weight', 'prompt_learner.meta_net.linear2.bias'}\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"outputs/cocoop/prompt_learner/model.pth.tar-100\" (epoch = 100)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:01<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 3,900\n",
      "* correct: 1,687\n",
      "* accuracy: 43.3%\n",
      "* error: 56.7%\n",
      "* macro_f1: 39.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on the New Classes.\n",
    "args.model_dir = \"outputs/cocoop\"\n",
    "args.output_dir = \"outputs/cocoop/new_classes\"\n",
    "args.subsample_classes = \"new\"\n",
    "args.load_epoch = 100\n",
    "args.eval_only = True\n",
    "coop_novel_acc = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1KdgiKFsowj"
   },
   "source": [
    "### **Q3. Analyzing the results of CoCoOp**\n",
    "Compare the results of CoCoOp with those of CoOp that we trained in Lab Session 4. Discuss possible reasons for the performance differences observed between CoCoOp and CoOp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion \n",
    "\n",
    "The performance difference between CoCoOp (Context Optimization Co-Op) and CoOp (Context Optimization) arises from the additional network architecture between them. CoOp is optimized for specific tasks by learning static context tokens during training. This approach allows it to perform exceptionally well in scenarios where the test set distribution is similar to the training data. On the other hand, CoCoOp employs a meta-learning approach to dynamically learn context for better generalization, particularly in cross-domain and few-shot settings. This design, while beneficial for generalization, may come at the cost of lower in-domain accuracy, as observed in your EuroSAT test results.\n",
    "\n",
    "One significant factor contributing to the observed performance gap is the trade-off between overfitting and generalization. CoOp’s static prompts are finely tuned for the dataset, leading to high accuracy when the test set closely resembles the training set. In contrast, CoCoOp’s dynamic prompts aim to generalize to unseen classes or domains, which may result in suboptimal performance on simpler datasets like EuroSAT. Additionally, the task complexity of EuroSAT, with its well-defined visual classes, may not fully benefit from CoCoOp’s added complexity. In this case, CoOp’s straightforward approach to prompt optimization is likely more effective.\n",
    "\n",
    "Another difference lies in the few-shot training process. CoCoOp relies on representative few-shot samples to adapt its prompts dynamically. If these few-shot samples are not diverse or representative of the test set, CoCoOp’s performance can suffer. Furthermore, the increased complexity of CoCoOp’s architecture, which includes a meta-network for learning dynamic prompts, can lead to challenges in optimization. This could result in suboptimal learning outcomes if hyperparameters, such as learning rates or context size, are not well-tuned."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
